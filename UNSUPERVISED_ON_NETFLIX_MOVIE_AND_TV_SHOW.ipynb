{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "OH-pJp9IphqM",
        "PIIx-8_IphqN",
        "BZR9WyysphqO",
        "YJ55k-q6phqO",
        "U2RJ9gkRphqQ",
        "x-EpHcCOp1ci",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "C74aWNz2AliB",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshikaPrajapati/PROJECT/blob/main/UNSUPERVISED_ON_NETFLIX_MOVIE_AND_TV_SHOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - NETFLIX MOVIES AND TV SHOWS CLUSTERING\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1  - ANSHIKA PRAJAPATI"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix uses data analysis and machine learning techniques such as clustering to group their content into similar categories, aiming to improve the user experience by providing personalized content recommendations to users based on their viewing history and preferences. This involves analyzing various characteristics of each title, such as genre, cast, and plot, and using algorithms to identify patterns and similarities.\n",
        "\n",
        "Clustering techniques such as k-means, hierarchical clustering, and principal component analysis (PCA) are used to group movies and TV shows with similar features into distinct groups, each representing a unique genre or category. This approach helps Netflix provide personalized recommendations to users based on their viewing history and preferences, leading to increased user engagement and satisfaction, which in turn can lead to increased retention and company revenue.\n",
        "\n",
        "Clustering enables Netflix to make data-driven decisions about content production and licensing by understanding underlying trends and patterns in user behavior. This helps the platform to optimize its content library and offer titles that are more likely to be successful with its user base, leading to increased customer retention and company revenue.\n",
        "\n",
        "In conclusion, Netflix Movies and TV Shows Clustering is a data-driven approach that relies on unsupervised machine learning algorithms to analyze and group its vast library of content into similar categories, providing users with personalized recommendations and improving the overall user experience. Clustering helps Netflix make informed decisions about content production and licensing, leading to increased customer retention and company revenue.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix, one of the largest streaming platforms globally, boasts a vast library of movies and TV shows. However, the abundance of options makes it difficult for users to locate content that suits their preferences. Thus, this project's objective is to use unsupervised learning techniques to cluster similar titles on Netflix. By grouping movies and TV shows with similar attributes, the project aims to provide users with more targeted recommendations, facilitating the discovery of new content that aligns with their interests.\n",
        "\n",
        "To achieve this goal, the project will analyze a Netflix title dataset, incorporating features like genre, cast, release year, plot summary, among others. Utilizing clustering algorithms like K-Means or Hierarchical clustering, the project intends to categorize movies and TV shows with comparable attributes.\n",
        "\n",
        "Ultimately, the project aims to develop a reliable clustering model that can group Netflix titles accurately based on their characteristics. This model can subsequently be used to enhance Netflix's content discovery algorithms or offer recommendations to users."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "## Data Maipulation Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# libraries used to process textual data\n",
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# libraries used to implement clusters\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "# Library of warnings would assist in ignoring warnings issued\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yF7Uzf1mEK8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# import drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Load Dataset\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DATA/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        ""
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset First Look\n",
        "df.head()\n",
        ""
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "mUJyQxFMIbzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Dataset Size\")\n",
        "print(\"Rows = {} and  Columns = {}\".format(df.shape[0], df.shape[1]))\n",
        ""
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar = False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset utilized for clustering Netflix movies and TV shows encompasses several features of the titles, including genre, rating, release year, duration, director, cast, and type. It comprises 7787 rows and 12 columns. Nonetheless, certain columns like director, cast, and country contain null values, which necessitate addressing during the data analysis phase."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables Description\n",
        "\n",
        "**show_id**: Unique ID for every Movie / Tv Show\n",
        "\n",
        "**type**: Identifier - A Movie or TV Show\n",
        "\n",
        "**title** : Title of the Movie / Tv Show\n",
        "\n",
        "**director** : Director of the Movie\n",
        "\n",
        "**cast** : Actors involved in the movie / show\n",
        "\n",
        "**country** : Country where the movie / show was produced\n",
        "\n",
        "**date_added** : Date it was added on Netflix\n",
        "\n",
        "**release_year** : Actual Releaseyear of the movie / show\n",
        "\n",
        "**rating** : TV Rating of the movie / show\n",
        "\n",
        "**duration** : Total Duration - in minutes or number of seasons\n",
        "\n",
        "**listed_in**: Genre\n",
        "\n",
        "**description** : The Summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \", i , \"is\" , df[i].nunique(), \".\")\n",
        ""
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summing null values\n",
        "print('Missing Data Count')\n",
        "df.isna().sum()[df.isna().sum() > 0].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "LAsFYb_dUVk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Missing Data Percentage')\n",
        "print(round(df.isna().sum()[df.isna().sum() > 0].sort_values(ascending=False)/len(df)*100,2))"
      ],
      "metadata": {
        "id": "iSd-C4B7UbOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The missing values in the 'director', 'cast', and 'country' columns can be replaced with the label 'Unknown'.\n",
        "     "
      ],
      "metadata": {
        "id": "RK4csusxUmAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['director']] = df[['director']].fillna('Unknown')\n",
        "df[['cast']]     = df[['cast']].fillna('Unknown')\n",
        "df[['country'] ] = df[['country']].fillna('Unknown')\n",
        ""
      ],
      "metadata": {
        "id": "nz07tvlqUtXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cannot replace missing values in the 'date_added' column.\n",
        "\n",
        " And since they constitute a small and relatively unimportant portion of the data.\n",
        "\n",
        "Therefore, we will exclude these values from our analysis."
      ],
      "metadata": {
        "id": "uejrK5ckU09Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['date_added'], inplace=True)"
      ],
      "metadata": {
        "id": "eP7NszJOVHM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "6xVJjMyDVLGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the missing values in the 'rating' column, we can impute them with the mode since this attribute is discrete.\n",
        "     "
      ],
      "metadata": {
        "id": "71RK62qsVldR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['rating'].fillna(value=df['rating'].mode()[0],inplace=True)"
      ],
      "metadata": {
        "id": "IIjF08f_VsYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "EcGV_w9FVv7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To simplify the analysis, we will choose the primary country and primary genre for each entry in country and listed in column.\n",
        "     \n"
      ],
      "metadata": {
        "id": "LzQcwyG2V3BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['country'] = df['country'].apply(lambda x: x.split(',')[0])\n",
        "df['listed_in'] = df['listed_in'].apply(lambda x: x.split(',')[0])"
      ],
      "metadata": {
        "id": "C70BRZLAV7u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will transform the 'duration' column in the dataframe by splitting the string value\n",
        "\n",
        "on whitespace delimiter and then converting it into an integer datatype.\n",
        "     \n"
      ],
      "metadata": {
        "id": "YxTDWlYtWDL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['duration'] = df['duration'].apply(lambda x: int(x.split()[0]))"
      ],
      "metadata": {
        "id": "Pl0QBTDXWK-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datatype of duration\n",
        "df.duration.dtype"
      ],
      "metadata": {
        "id": "Al95_VPtWRAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns and data types\")\n",
        "pd.DataFrame(df.dtypes).rename(columns = {0:'dtype'})"
      ],
      "metadata": {
        "id": "ee08ktJpWVn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert timestamp to datetime format to fetch the other details\n",
        "df[\"date_added\"] = pd.to_datetime(df['date_added'])\n",
        ""
      ],
      "metadata": {
        "id": "2MDeuC4_WbDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#addding new column to dataframe such as 'month_added'and 'year_added' to gain more insights from the data\n",
        "df['year_added'] = df['date_added'].dt.year\n",
        "df['month_added'] = df['date_added'].dt.month\n",
        ""
      ],
      "metadata": {
        "id": "HFHsFDk4WfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the values in the rating column\n",
        "# Create a dictionary to map the current ratings to new ratings\n",
        "rating_map = {'TV-MA':'Adults',\n",
        "'R':'Adults',\n",
        "'PG-13':'Teens',\n",
        "'TV-14':'Young Adults',\n",
        "'TV-PG':'Older Kids',\n",
        "'NR':'Adults',\n",
        "'TV-G':'Kids',\n",
        "'TV-Y':'Kids',\n",
        "'TV-Y7':'Older Kids',\n",
        "'PG':'Older Kids',\n",
        "'G':'Kids',\n",
        "'NC-17':'Adults',\n",
        "'TV-Y7-FV':'Older Kids',\n",
        "'UR':'Adults'}\n",
        "# Replace the current ratings with the new ratings using the mapping dictionary\n",
        "df['rating'].replace(rating_map,inplace=True)\n",
        "# Print the unique values in the 'rating' column to verify that the changes have been made\n",
        "print(df['rating'].unique())\n",
        ""
      ],
      "metadata": {
        "id": "wdU0DKWzWiVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "CVkzVoKpWngn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)The label 'Unknown' was used to replace missing values in the 'director', 'cast', and 'country' columns.\n",
        "\n",
        "2)The mode was used for imputing missing values in the 'rating' column. For simplifying the analysis, the primary country and primary genre were selected for each entry in the dataframe.\n",
        "\n",
        "3)The 'duration' column in the dataframe was transformed by converting the string value to an integer datatype after splitting it on a whitespace delimiter.\n",
        "\n",
        "4)To extract additional details, the timestamp in the 'date_added' column was converted to datetime format, and new columns such as 'month_added' and 'year_added' were added to the dataframe.\n",
        "\n",
        "5)A dictionary was created to map the current ratings to new ratings, which were then used to replace the values in the rating column."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Chart - 1 visualization code\n",
        "\n",
        "type_counts = df['type'].value_counts()        # Count the occurrences of each unique value in the 'type' column\n",
        "plt.bar(type_counts.index, type_counts.values) # Create a bar chart of the type counts\n",
        "plt.xlabel('Content Type')                     # Add labels and a title to the chart\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.title('Distribution of Movies and TV Shows in Netflix Dataset')\n",
        "plt.show()                                     # Show the chart\n",
        ""
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For displaying the distribution of categorical data, such as the number of movies and TV shows in the Netflix dataset, a bar chart is a suitable option. It facilitates simple comparison between categories and offers a clear visualization of the overall content type distribution in the dataset, making it a fitting choice for this particular dataset and research inquiry."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Netflix dataset contains a higher count of movies compared to TV shows, indicating a preference towards movies on the platform. However, it is important to note that TV shows still make up a significant portion of the dataset, suggesting that Netflix also invests in producing and acquiring TV shows for its platform."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The bar chart provides valuable insights for Netflix's business decisions. Understanding that movies are the majority of the content in the dataset and that Netflix has a preference towards them can inform decisions related to content production and acquisition. This information may lead to a strategic allocation of resources towards producing and acquiring more movies, potentially attracting more viewers and subscribers."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Filter out the rows where the director is unknown, count the number of shows for each director, and plot the top 10\n",
        "top_directors = df.loc[df['director'] != 'Unknown', 'director'].value_counts().nlargest(10)\n",
        "plt.figure(figsize=(15,6))\n",
        "colors = sns.color_palette('pastel', n_colors=10)\n",
        "plt.bar(top_directors.index, top_directors.values, color=colors)\n",
        "plt.title('Top 10 Directors by Number of Shows Directed')\n",
        "plt.xlabel('Director')\n",
        "plt.ylabel('Number of Shows')\n",
        "plt.xticks(rotation=15)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is an effective way to visualize the top 10 directors in the Netflix dataset, ranked by the number of shows they have directed. This chart is useful for understanding the relationship between directors and the amount of content they have contributed to Netflix, and it provides insights into the most prominent directors on the platform. Additionally, the use of color in the chart enhances its visual appeal and readability."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Raul Campos and Jan Suter are the top directors in the Netflix dataset, having directed the highest number of shows at 18.\n",
        "\n",
        "2)Marcus Raboy is the second most popular director, having directed 16 shows.\n",
        "\n",
        "3)The majority of the top 10 directors have directed between 7-11 shows on Netflix.\n",
        "\n",
        "4)With the exception of David Dhawan from India, all of the top 10 directors are from the US."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The valuable insights gained from analyzing the top directors and their past work on Netflix may guide decisions related to content production and acquisition."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 CAST (Univariate Analysis)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "filtered_df = df[~(df['cast']=='Unknown')]                       # Filtering out unknown cast members\n",
        "split_cast = filtered_df['cast'].str.split(', ', expand=True)    # split remaining cast into separate values\n",
        "cast_values = split_cast.stack().reset_index(level=1, drop=True)\n",
        "top_10_actors = cast_values.value_counts().nlargest(10)          #the top 10 actors by number of shows\n",
        "top_10_actors.plot(kind='bar', figsize=(13,5))                   # Create a bar chart\n",
        "plt.title('Top 10 Actors by Number of Shows', fontsize=14)       # Set chart title\n",
        "plt.ylabel('Number of Shows', fontsize=12)                       #y-axis label\n",
        "plt.xticks(rotation=15)                                          #x-axis label with rotation\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is used to display the top 10 actors in the Netflix dataset, based on the number of shows they appeared in. This chart is useful in examining the association between actors and the number of shows they have been part of on Netflix, and it can offer significant insights into the most in-demand actors on the platform."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals valuable insights about the popularity of actors on Netflix, including:\n",
        "\n",
        "Anupam Kher is the most frequently appearing actor on the platform, having appeared in 42 shows in the dataset.\n",
        "\n",
        "Shah Rukh Khan follows closely behind, having appeared in 35 shows.\n",
        "\n",
        "The majority of the top 10 actors have appeared in 25-30 shows on Netflix.\n",
        "\n",
        "With the exception of Takahiro Sakurai and Yuki Kaji from Japan, the top 10 actors are primarily from India.\n",
        "\n",
        "These insights can inform decisions related to content production and acquisition by providing information about the most popular actors on Netflix and their past work."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "1)The knowledge of the most popular actors on the platform can aid Netflix in acquiring or producing content that showcases these actors, potentially resulting in increased viewership and engagement on their platform.\n",
        "\n",
        "2)These insights may assist in identifying the target audience for various titles, as different actors may appeal to different demographics, enabling Netflix to cater to their audience's preferences better.\n",
        "\n",
        "3)The data can also help Netflix recognize patterns and preferences among its user base, which can inform decisions regarding content acquisition and production."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 LISTED IN (Univariate Analysis)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "top_genres = df[\"listed_in\"].value_counts().head(10)\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(top_genres, labels=top_genres.index, autopct='%1.1f%%', pctdistance=0.8, labeldistance=1.1,\n",
        "        radius=1.2, wedgeprops=dict(width=0.5), startangle=90,\n",
        "        textprops=dict(color=\"black\", fontsize=12), counterclock=False)\n",
        "plt.title('Top 10 Genres', fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the pie chart because it effectively illustrates the distribution of the top 10 genres in the Netflix dataset. Pie charts are ideal for displaying relative proportions or percentages of categorical data, making them a suitable choice for this analysis."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart effectively illustrates the proportion of each genre among the top 10 genres in the Netflix dataset. By analyzing this chart, we can conclude that dramas are the most prevalent genre, followed by comedies and documentaries. The chart also highlights that the top 10 genres constitute a considerable portion of the dataset, emphasizing the importance of these genres in the streaming industry."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "By using the insights gained from the chart, Netflix can gain a deeper understanding of their audience's content preferences. This information can be used to make data-driven decisions about which types of content to acquire and produce, ultimately leading to increased viewership and revenue. The pie chart provides valuable information about the most popular genres in the Netflix dataset, allowing Netflix to tailor their content strategy to better align with their audience's preferences."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 RATING (Univariate Analysis)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "df_rating = df['rating'].value_counts()\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(df_rating.values, labels=df_rating.index,\n",
        "        autopct='%1.1f%%',startangle=90, counterclock=False)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use a pie chart to illustrate the distribution of content ratings in the Netflix dataset as it is an efficient way to depict the proportion of data in each category. The chart clearly displays the percentage of titles in each rating category, making it easy to observe that the majority of titles in the dataset are TV-MA, followed by TV-14 and TV-PG."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart illustrates the distribution of content ratings in the Netflix dataset. As per the chart, the majority of titles in the dataset have been rated as Adults (TV-MA - Mature Audiences), which comprises nearly 47% of all titles. Additionally, Young Adults (TV-14 - Parents Strongly Cautioned) and Older Kids (TV-PG - Parental Guidance Suggested) are the next most common ratings, representing approximately 25% and 17% of titles, respectively."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "By gaining insights into the distribution of content ratings in the Netflix dataset, businesses can make informed decisions about their content acquisition and creation strategies. The fact that the TV-MA rating is the most common suggests a strong demand for mature content on the platform, which could guide decisions about what types of content to acquire or produce to cater to this audience. Additionally, understanding the distribution of ratings can help businesses tailor their marketing efforts to different demographics and promote content that is likely to be popular with specific age groups."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 COUNTRY (Univariate Analysis)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Get the top 10 countries with the highest number of movies and TV shows in the dataset\n",
        "top_countries = df.loc[df['country'] != 'Unknown', 'country'].value_counts().nlargest(10)\n",
        "plt.figure(figsize=(15,5))\n",
        "colors = sns.color_palette('deep', n_colors=10)\n",
        "plt.barh(top_countries.index, top_countries.values, color=colors) # Plot a horizontal bar chart\n",
        "plt.title('Top 10 Countries with the Highest Number of Shows')\n",
        "plt.xlabel('Number of Shows')\n",
        "plt.ylabel('Country')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage share of shows by the top 3 and top 10 countries\n",
        "top_3_share = top_countries.nlargest(3).sum() / len(df) * 100\n",
        "top_10_share = top_countries.sum() / len(df) * 100\n",
        "\n",
        "# Print the percentage shares\n",
        "print(f\"The top 3 countries account for {top_3_share:.2f}% of shows in the dataset.\")\n",
        "print(f\"The top 10 countries account for {top_10_share:.2f}% of shows in the dataset.\")\n",
        ""
      ],
      "metadata": {
        "id": "GZ6COSJmZai0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart because it displays the top 10 countries with the highest number of movies and TV shows in the Netflix dataset, which provides valuable information for companies seeking to enter or expand in the global streaming market."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the chart, the United States dominates the production of movies and TV shows in the Netflix dataset with over 2,500 titles. India, the United Kingdom, and Canada are the next highest producing countries, each with around 500-1000 titles. The top 10 countries, including France, Japan, and Spain, account for 73.19% of shows, while the top 3 countries (USA, India, UK) contributing 56.69%. This information is valuable for businesses looking to understand the global streaming market and identify potential areas for expansion or investment."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The insights gained from this chart can have a significant impact on businesses in the streaming industry. Understanding that the US is the largest producer of movies and TV shows can help companies plan their content acquisition and marketing strategies. Furthermore, the knowledge that the top 3 countries account for more than half of the shows in the dataset can assist companies in targeting these markets to increase their viewership and expand their operations.\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 MONTH ADDED (Univariate Analysis)\n",
        "\n"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Plotting the Countplot\n",
        "plt.figure(figsize=(10,8))\n",
        "ax = sns.countplot(x='month_added', data=df)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This count plot, created using the seaborn library, displays the number of TV shows and movies added to Netflix for each month in the dataset. The plot provides a clear and concise visualization of the trends in content additions over time."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Count Plot provides valuable insights into the frequency of TV shows and movies added to Netflix for each month in the dataset. By analyzing this chart, we can identify the months with the highest and lowest number of additions to the platform, which can help inform decisions related to content acquisition and release schedule. Specifically, the chart indicates that the highest number of movies were added between October and January, while the number of additions was relatively low during the rest of the year. This information could be useful for Netflix to plan their content acquisition strategy and ensure a consistent flow of new releases throughout the year."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The chart provides valuable insights that could help Netflix identify areas for improvement. For instance, if there are months with low numbers of additions, Netflix may consider acquiring more content during those months or releasing more original content to fill the gaps. On the other hand, if there are months with a high number of additions, Netflix may need to adjust its release schedule to avoid overcrowding the platform with too much content at once. These actions could help ensure a consistent flow of content and prevent user fatigue, ultimately improving user engagement and satisfaction."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 TV SHOW VS NO OF SEASONS (Bivariate Analysis)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "tv_shows = df[df['type'] == 'TV Show']                    # Filter the dataframe to only include TV shows\n",
        "plt.figure(figsize=(15, 7))                               # Create a histogram of the number of seasons per TV show\n",
        "plt.hist(tv_shows['duration'], bins=20, edgecolor='black')\n",
        "plt.xlabel('Number of Seasons', fontsize=12)\n",
        "plt.ylabel('Number of Shows', fontsize=12)\n",
        "plt.title('Number of Seasons per TV Show Distribution')\n",
        ""
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart represents a histogram that displays how many TV shows in the dataset have a particular number of seasons. This information can be used to identify the most frequent number of seasons in TV shows and the spread of data."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Most TV shows have one to three seasons.\n",
        "\n",
        "2)Few TV shows have more than 10 seasons."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "1)Understanding that shorter seasons may be more popular and successful could guide Netflix's decisions on the number of seasons to order for new shows.\n",
        "\n",
        "2)Knowing the distribution of the number of seasons per TV show can be valuable in negotiating the length of a show with production companies."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 RELEASE YEAR VS TYPE (Bivariate Analysis)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df_release_year = df.groupby(['release_year', 'type'])['show_id'].count().reset_index()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.lineplot(data=df_release_year, x='release_year', y='show_id', hue='type')\n",
        "plt.title('Number of Movies and TV Shows Released Each Year')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I selected this chart because it shows how the number of movies and TV shows added to Netflix has changed over time. By using a line plot, this chart makes it simple to compare the trends in the release of movies and TV shows each year. Additionally, the chart's color coding allows for a quick visual comparison of the two types of content. Overall, this chart can be useful for understanding the relationship between the year of release and the number of movies and TV shows added to Netflix."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)This chart provides insights into the trend of media content production over the years by showing the number of movies and TV shows released each year.\n",
        "\n",
        "2)The line plot demonstrates a significant increase in the number of movies produced from the mid-2000s to 2020, while the number of TV shows produced has also increased but not as much as movies.\n",
        "\n",
        "3)A dip in movie production in 2020 is also visible in the chart, which may be attributed to the COVID-19 pandemic."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The insights gained from the chart can have a significant impact on the business. It can provide valuable information for content creators, streaming platforms, and investors. For instance, the increased production of movies suggests a change in audience preferences towards movies, which can guide content creation and platform offerings. However, the dip in movie production due to the COVID-19 pandemic can have a negative impact, leading to a shortage of new content for streaming platforms and reduced revenue for content creators."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 TYPE VS RELEASE YEAR (Bivariate Analysis)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "df.release_year.value_counts()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe from the visualization that the number of shows released on Netflix has increased significantly in recent years,\n",
        "\n",
        "indicating that Netflix has gained more popularity in recent times."
      ],
      "metadata": {
        "id": "5tnONEUhdaRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "filtered_df = df[df['release_year'] >= 2008]\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.countplot(x='release_year', data=filtered_df, hue='type', order=range(2008, 2022))\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Shows')\n",
        "plt.title('Number of Shows Released Each Year Since 2008 on Netflix')\n",
        ""
      ],
      "metadata": {
        "id": "IVz4j4p3djp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for selecting this chart is that it displays the trend in the yearly release of TV shows and movies since 2008, with a clear comparison of the two types of content."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.This chart shows an upward trend in the yearly release of TV shows since 2008.\n",
        "\n",
        "2.TV show releases have been steady over the years, with slight fluctuations.\n",
        "\n",
        "3.The chart suggests a shift towards producing more original movies content on Netflix, based on the difference in the number of movies and TV shows released each year."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Insights from this chart can positively impact Netflix's business by providing information on content production trends, allowing them to make strategic decisions regarding future content type and quantity. This chart does not provide any insights that could lead to negative growth."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 COUNTRY VS SHOW ID (Bivariate Analysis)"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# group the data by country and type, and count the number of shows\n",
        "df_country = df.groupby(['country', 'type'])['show_id'].count().reset_index()\n",
        "df_country = df_country.sort_values(by='show_id', ascending=False)            # sort the data in descending order\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.barplot(data=df_country[:20], x='country', y='show_id', hue='type')       # plot a bar chart of the top 20 countries\n",
        "plt.xticks(rotation=90)\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Top 20 Countries by Number of Shows on Netflix')\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Number of Shows')\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected this chart for its ability to provide valuable insights into the top 20 countries with the highest number of shows on Netflix, categorized by country and show type. These insights can be leveraged to inform content acquisition and localization strategies for Netflix."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart displays the top 20 countries with the highest number of shows on Netflix, providing valuable information for content acquisition and localization strategies. The United States has the largest amount of content available on Netflix with over 2,000 shows, followed by India with over 800 shows and the United Kingdom with over 300 shows. In terms of show types, most of the content in these countries are movies, with the United Kingdom having a relatively equal distribution between movies and TV shows."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Analyzing the chart can lead to a positive business impact by guiding content acquisition and localization strategies. For instance, companies can prioritize acquiring content from countries with the highest number of shows on Netflix to expand their content library in a specific region. Additionally, localization strategies such as dubbing or subtitling can be developed for these regions to increase viewership and revenue."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 CAST VS TV SHOW (Bivariate Analysis)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Selecting TV shows with known cast information\n",
        "tv_shows = df[(df['type'] == 'TV Show') & ~(df['cast'] == 'Unknown')]\n",
        "# Counting the number of TV shows each actor has appeared in\n",
        "actor_counts = tv_shows['cast'].str.split(', ').explode().value_counts()\n",
        "top_actors = actor_counts.head(10)             # Selecting the top 10 actors with the most TV show appearances\n",
        "plt.figure(figsize=(15, 6))                    # Creating a horizontal bar plot of the top actors\n",
        "plt.barh(top_actors.index, top_actors.values, color='purple')\n",
        "plt.xlabel('Number of TV Shows', fontsize=12)\n",
        "plt.ylabel('Actor Name for TV Shows', fontsize=12)\n",
        "plt.title('Actors with the Most TV Show Appearances', fontsize=14)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The horizontal bar plot represents the top 10 actors with the highest number of appearances in TV shows, providing insights into the most frequently cast actors in this medium. This chart could be valuable for individuals in the entertainment industry or those interested in popular culture who want to stay updated on which actors are in demand for TV show roles."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart depicts the top 10 actors with the highest number of TV show appearances on Netflix, indicating that Takahiro Sakurai has the most appearances, followed by Yuki Kaji and Daisuke Ono. This information could be leveraged to identify popular actors that could attract audiences for new TV show releases, making the insights valuable for content creators and streaming platforms."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The insights gained from this chart could positively impact the selection of actors for TV shows. Casting popular actors could potentially increase viewership and have a positive impact on business. However, it is important to note that an actor's popularity alone is not a guarantee of success, as the quality of the TV show is also a significant factor."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 WORDCLOUD"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Join all the movie descriptions together into a single string\n",
        "comment_words = ' '.join(df['description'].astype(str).str.lower())\n",
        "stopwords = set(STOPWORDS)                                   # Define the stopwords\n",
        "wordcloud = WordCloud(width=800, height=800,\n",
        "                      background_color='black',\n",
        "                      stopwords=stopwords,\n",
        "                      min_font_size=10).generate(comment_words)\n",
        "plt.figure(figsize=(10,5), facecolor=None)                   # Plot the word cloud\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a word cloud using the descriptions of shows and movies in the Netflix dataset, providing a quick and easy way to visualize the most frequent words and themes in the content. This insight can be useful for understanding the overall genre and content themes available on Netflix."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud generated from the Netflix dataset provides valuable insights into the frequently occurring words and phrases in the descriptions of shows and movies. This information can be used to identify the popular themes and genres among Netflix users, and also to discover unique keywords and phrases for marketing purposes. The most common words in the word cloud include \"life\", \"family\", \"friend\", \"love\", and others."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "The insights gained from the word cloud can benefit Netflix by enabling them to understand their users' interests better and personalize their content to meet those interests. This can result in a positive business impact by allowing Netflix to create more effective marketing campaigns and enhance the overall user experience."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,8))\n",
        "correlation = df.corr()\n",
        "sns.heatmap((correlation), annot=True, cmap=sns.color_palette(\"mako\", as_cmap=True))\n",
        ""
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap provides a visual representation of the correlation coefficients among the numerical columns in the Netflix dataset. Positive correlation is represented by lighter colors, while negative correlation is represented by darker colors. The values of the correlation coefficients are also displayed within each cell of the heatmap, thanks to the annotation parameter being set to True."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This heatmap is a useful tool for identifying the relationships between different variables in the Netflix dataset. By analyzing the heatmap, we can easily see which variables have a strong positive or negative correlation with each other, which can help in making predictions and building machine learning models.\n",
        "\n",
        "Overall, this heatmap can provide valuable insights into the relationships between different variables in the Netflix dataset.\n",
        "\n",
        "1.   We can see that duration and release year are negatively correlated by 24%.\n",
        "\n",
        "2.   year added and release year are positively correlated by 10%.\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot provides a comprehensive view of the numerical variables in the dataset, allowing for a deeper exploration of the relationships and patterns between each pair of variables. With scatter plots and histograms, it can reveal insights into the distribution and correlations of the data, which can aid in making informed decisions and identifying trends in the dataset."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.The diagonal plots in the pair plot show the distribution and range of each variable. The duration of movies and TV shows appears to be concentrated in certain ranges.\n",
        "\n",
        "2.The pair plot also displays scatter plots of each pair of variables and their correlation coefficients. There appears to be a positive correlation between release year and duration, suggesting newer movies and TV shows tend to be longer.\n",
        "\n",
        "3.Outliers in the data can be identified from the scatter plots in the pair plot. One movie in the dataset appears to have an unusually long duration compared to the rest."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1**: How does the average duration of movies compare to TV shows on Netflix\n",
        "\n",
        "**Hypothesis 2**: How does the average number of seasons for TV shows on Netflix vary between those produced in the United States and those produced outside of the United States\n",
        "\n",
        "**Hypothesis 3**: The quantity of TV shows added to Netflix has grown progressively over time."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 How does the average duration of movies compare to TV shows on Netflix"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis(H0) - There is no significant difference in the average duration of movies and TV shows on Netflix.\n",
        "\n",
        "Alternative Hypothesis(H1) - There is a significant difference in the average duration of movies and TV shows on Netflix."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extract the durations of movies and TV shows from the dataset\n",
        "movie_durations = df[df['type'] == 'Movie']['duration']\n",
        "tv_show_durations = df[df['type'] == 'TV Show']['duration']\n",
        "\n",
        "# Perform two-sample t-test\n",
        "stat, p = ttest_ind(movie_durations, tv_show_durations, equal_var=False)\n",
        "\n",
        "# Print the test statistic and p-value\n",
        "print(\"Two-sample t-test statistic:\", stat)\n",
        "print(\"p-value:\", p)\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print(\"Failed to reject null hypothesis.\")\n",
        "else:\n",
        "    print(\"Reject null hypothesis.\")\n",
        ""
      ],
      "metadata": {
        "id": "wk7V8UGAg0AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "There is a significant difference in the average duration of movies and TV shows on Netflix.\n"
      ],
      "metadata": {
        "id": "X74n9tMRg7Hy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the two-sample t-test as our statistical test since we are comparing the means of two independent samples (movie durations and TV show durations) and want to determine if the difference between the sample means is statistically significant or simply due to chance. The test's p-value provides insight into the significance of the difference."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was used to compare the means of movie and TV show durations, assuming that the samples are independent and normally distributed. The test also assumes that the variances of the two samples are not equal, which is likely due to the differences in content between movies and TV shows."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 How does the average number of seasons for TV shows on Netflix vary between those produced in the United States and those produced outside of the United States"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis : There is no significant difference in the average number of seasons for TV shows on Netflix between those produced in the United States and those produced outside of the United States.\n",
        "\n",
        "Alternate hypothesis : There is a significant difference in the average number of seasons for TV shows on Netflix between those produced in the United States and those produced outside of the United States."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extract the number of seasons for TV shows produced in the US and outside the US\n",
        "# Extract the number of seasons for TV shows produced in the US and outside the US\n",
        "us_shows = df[(df['type'] == 'TV Show') & (df['country'] == 'United States')]\n",
        "us_shows_seasons = us_shows['duration'].apply(lambda x: int(x.split(' ')[0]) if isinstance(x, str) and 'season' in x else 0)\n",
        "\n",
        "non_us_shows = df[(df['type'] == 'TV Show') & (df['country'] != 'United States')]\n",
        "non_us_shows_seasons = non_us_shows['duration'].apply(lambda x: int(x.split(' ')[0]) if isinstance(x, str) and 'season' in x else 0)\n",
        "\n",
        "# Perform two-sample t-test\n",
        "stat, p = ttest_ind(us_shows_seasons, non_us_shows_seasons, equal_var=False)\n",
        "\n",
        "# Print the test statistic and p-value\n",
        "print(\"Two-sample t-test statistic:\", stat)\n",
        "print(\"p-value:\", p)\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p > alpha:\n",
        "    print(\"Failed to reject null hypothesis.\")\n",
        "else:\n",
        "    print(\"Reject null hypothesis.\")"
      ],
      "metadata": {
        "id": "rB5cyF5chggn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a significant difference in the average number of seasons for TV shows on Netflix\n",
        "\n",
        "between those produced in the United States and those produced outside of the United States.\n",
        "     "
      ],
      "metadata": {
        "id": "kMW6gwbXhla4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis involves using a two-sample t-test, which is a statistical test used to compare the means of two independent samples and determine whether they are significantly different from each other."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized a two-sample t-test to compare the mean number of seasons between TV shows produced in the US and those produced outside the US. The choice of this test was based on our goal of identifying if there is a significant difference between the two groups' means. We also took into consideration the potential inequality of variances between the two groups by setting the equal_var parameter to False in the ttest_ind() function."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 The quantity of TV shows added to Netflix has grown progressively over time."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: The mean number of TV shows added to Netflix per year has not changed over time.\n",
        "\n",
        "Alternative hypothesis: The mean number of TV shows added to Netflix per year has increased over time."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extract the year from the date_added column\n",
        "df['year_added'] = pd.DatetimeIndex(df['date_added']).year\n",
        "\n",
        "# Extract the number of TV shows added to Netflix each year\n",
        "tv_shows = df[df['type'] == 'TV Show']\n",
        "tv_shows_by_year = tv_shows.groupby('year_added').size()\n",
        "\n",
        "# Perform a linear regression to test for a positive slope (i.e., an increase over time)\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(tv_shows_by_year.index, tv_shows_by_year)\n",
        "\n",
        "# Print the p-value\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the result\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"Failed to reject null hypothesis.\")\n",
        "else:\n",
        "    print(\"Reject null hypothesis.\")"
      ],
      "metadata": {
        "id": "90DKHCXkiEvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code uses the stats.linregress function to perform a linear regression that tests for a positive slope in the number of TV shows added to Netflix each year. The resulting p-value measures the strength of evidence against the null hypothesis that the slope is zero, indicating no increase over time, and in favor of the alternative hypothesis that there is a positive slope indicating an increase over time."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code utilizes a linear regression with a hypothesis test on the slope coefficient to test for a trend over time. This is suitable because the objective is to model the relationship between the year and the number of TV shows added to Netflix, and a linear regression is well-suited for this purpose. The resulting p-value provides evidence against the alternative hypothesis that there is a positive trend."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have already handled all the missing values in the data wrangling section."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "sns.boxplot(data=df)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need to handle the outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "df['organized'] =(df['description'].astype(str) + ' ' +\n",
        "                  df['listed_in'].astype(str)   + ' ' +\n",
        "                  df['rating'].astype(str)      + ' ' +\n",
        "                  df['cast'].astype(str)        + ' ' +\n",
        "                  df['country'].astype(str)     + ' ' +\n",
        "                  df['director'].astype(str))\n",
        ""
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.organized[0]"
      ],
      "metadata": {
        "id": "7uXF_HiSjFsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['Lower_casing']= df['organized'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Lower_casing[0]"
      ],
      "metadata": {
        "id": "qnwT94ESjPGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "[punc for punc in string.punctuation]"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_punctuation(text):\n",
        "    # remove punctuation from text\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "TGCis2FOjYYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['cleaned_text'] = df['Lower_casing'].apply(remove_punctuation)"
      ],
      "metadata": {
        "id": "Q_L06ncRjb5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.cleaned_text[0]"
      ],
      "metadata": {
        "id": "LakoK7FEjfcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "1eHDIM63jmm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaned(x):\n",
        "    return re.sub(r\"[^a-zA-Z ]\", \"\", str(x))\n",
        "\n",
        "def remove_urls(text):\n",
        "    cleaned_text = re.sub(r'http\\S+', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_digits(text):\n",
        "    cleaned_text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "A73mxQKfjqDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['removed_words']  = df['cleaned_text'].apply(cleaned)\n",
        "df['removed_url']    = df['removed_words'].apply(remove_urls)\n",
        "df['removed_digits'] = df['removed_url'].apply(remove_digits)"
      ],
      "metadata": {
        "id": "LZhQu3x9jtzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "\n",
        "df['removed_stopwords'] = df['removed_digits'].apply(remove_stopwords)\n",
        "print(df.removed_stopwords[0])\n",
        ""
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespaces(text):\n",
        "    cleaned_text = text.strip()\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['removed_whitespaces']=df['removed_stopwords'].apply(remove_whitespaces)\n",
        "df['removed_whitespaces'].head()"
      ],
      "metadata": {
        "id": "Oh4nyXjOkra8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        ""
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokenized'] = df['removed_whitespaces'].apply(tokenize_text)"
      ],
      "metadata": {
        "id": "bFGLfk10lQzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokenized'].head()"
      ],
      "metadata": {
        "id": "bQW8TjE2lWlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "def normalize_text(tokens):\n",
        "    stemmer = SnowballStemmer('english')          # apply stemming to tokens\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    lemmatizer = WordNetLemmatizer()              # apply lemmatization\n",
        "    normalized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "    normalized_text = ' '.join(normalized_tokens) # join normalized tokens\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "Cy8gtwChlk2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['normalized'] = df['tokenized'].apply(normalize_text)"
      ],
      "metadata": {
        "id": "BJ9P6Up9lv9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['normalized'].head()"
      ],
      "metadata": {
        "id": "RWUwnV-glyvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and lemmatization are two techniques used in NLP to normalize text by converting words to their base or canonical form. They are used to reduce the complexity of data, improve search results, decrease the vocabulary size, and enhance model accuracy. Stemming is an aggressive technique that chops off word endings, while lemmatization is a more sophisticated approach that considers the morphology of words to bring them to their base form."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "\n",
        "new_df = df[['title', 'normalized']]\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "t_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "x= t_vectorizer.fit_transform(new_df['normalized'])\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "id": "WWJqpHIemJHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the TF-IDF (Term Frequency-Inverse Document Frequency) text vectorization technique. This technique is commonly used for text classification and information retrieval tasks. It assigns weights to each word in the document based on its frequency and rarity across the corpus. This helps to highlight the most important words in the document and down-weight the common words that do not provide much useful information for the analysis."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Needed"
      ],
      "metadata": {
        "id": "bFDWKMMLrPnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not Needed"
      ],
      "metadata": {
        "id": "oMlJkq4RrbA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "YES\n",
        "\n",
        "As the number of features (words in this case) is high, it is useful to apply dimensionality reduction to simplify the dataset and improve computational efficiency."
      ],
      "metadata": {
        "id": "cDX7HFn1rwGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(x.toarray())"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
        "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org"
      ],
      "metadata": {
        "id": "o_aRzcTwxQTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the cumulative explained variance ratio\n",
        "cumulative_var_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot the cumulative explained variance ratio versus the number of components\n",
        "plt.figure(figsize=(5, 5), dpi=120)\n",
        "plt.plot(cumulative_var_ratio)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zC1d8NWtsrDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_tuned = PCA(n_components=0.95)\n",
        "x_dense = x.toarray()\n",
        "pca_tuned.fit(x_dense)\n",
        "x = pca_tuned.transform(x_dense)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "15EsY2dxs0EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "S8nOC_NQs3eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which dimensionality reduction technique have you used and why?\n",
        "(If dimensionality reduction done on dataset.)\n",
        "\n",
        "This code utilizes PCA (Principal Component Analysis) for the purpose of dimensionality reduction. Initially, a PCA model is fitted to the data without specifying the number of components, allowing us to obtain the explained variance ratio for each component. This information is used to determine the appropriate number of components to retain.\n",
        "\n",
        "Once the number of components is determined, a new PCA model is created with n_components set to 0.95, indicating that we want to retain enough components to explain 95% of the variance in the data. Finally, the original data is transformed using the transform method to produce a reduced-dimensionality dataset.\n",
        "\n",
        "The ultimate goal of this code is to effectively reduce the dimensionality of the text data while preserving important information, thereby improving the efficiency of subsequent analysis."
      ],
      "metadata": {
        "id": "xP7fS3ONuubI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "def evaluate_clustering_model(model, X, y_predict):\n",
        "    \"\"\"\n",
        "    Evaluate a clustering model and print the results.\n",
        "    & Returns\n",
        "    Model evaluation result\n",
        "    \"\"\"\n",
        "    # Calculate the number of clusters and evaluation metrics\n",
        "    n_clusters = len(set(y_predict))\n",
        "    S_score = silhouette_score(X, y_predict)\n",
        "    CH_score = calinski_harabasz_score(X, y_predict)\n",
        "    DB_score = davies_bouldin_score(X, y_predict)\n",
        "\n",
        "    # Print the evaluation results\n",
        "    print(f\"Number of clusters: {n_clusters}\")\n",
        "    print(f\"Silhouette score: {S_score:.4f}\")\n",
        "    print(f\"Calinski-Harabasz score: {CH_score:.4f}\")\n",
        "    print(f\"Davies-Bouldin score: {DB_score:.4f}\")\n",
        "\n",
        "    # Create a dictionary to store the evaluation scores\n",
        "    scores_dict = {\"silhouette_score\": S_score,\n",
        "                   \"calinski_harabasz_score\": CH_score,\n",
        "                   \"davies_bouldin_score\": DB_score}\n",
        "\n",
        "    # Create a dataframe to display the evaluation results\n",
        "    df_eval = pd.DataFrame({\"Evaluation Metric\": [\"Silhouette Score\",\n",
        "                                                  \"Calinski-Harabasz Score\",\n",
        "                                                  \"Davies-Bouldin Score\"],\n",
        "                                     \"Score\": [S_score, CH_score, DB_score]})\n",
        "\n",
        "    # Print the dataframe\n",
        "    print(tabulate(df_eval, headers=\"keys\", tablefmt=\"grid\"))\n",
        "\n",
        "    # Return the evaluation results\n",
        "    return {\"n_clusters\": n_clusters,\n",
        "            \"silhouette_score\": S_score,\n",
        "            \"calinski_harabasz_score\": CH_score,\n",
        "            \"davies_bouldin_score\": DB_score}\n",
        "\n",
        "def plot_clustering_scores(scores_dict):\n",
        "    \"\"\"\n",
        "    Plot the clustering evaluation scores using a bar chart.\n",
        "    \"\"\"\n",
        "    # Extract the scores from the dictionary\n",
        "    scores = [scores_dict[\"silhouette_score\"], scores_dict[\"calinski_harabasz_score\"], scores_dict[\"davies_bouldin_score\"]]\n",
        "    labels = [\"Silhouette\", \"Calinski-Harabasz\", \"Davies-Bouldin\"]\n",
        "\n",
        "    # Plot the scores as a bar chart\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.bar(labels, scores, color=[\"tab:blue\", \"tab:orange\", \"tab:green\"])\n",
        "\n",
        "    # Add labels and titles\n",
        "    ax.set_xlabel(\"Evaluation Metric\")\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_title(\"Clustering Evaluation Scores\")\n",
        "\n",
        "    # Set the y-axis limits to the range of the scores\n",
        "    ax.set_ylim([np.min(scores) - 0.1, np.max(scores) + 0.1])\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 1) K-MEANS CLUSTERING**"
      ],
      "metadata": {
        "id": "vWkrACCjwDPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just guessing and checking by k=3"
      ],
      "metadata": {
        "id": "qdBAV3BQw5OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3, max_iter = 50)\n",
        "kmeans.fit(x)\n",
        ""
      ],
      "metadata": {
        "id": "kbeKXSVJw8dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
        "\n",
        "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org."
      ],
      "metadata": {
        "id": "7ARMxJ1Z1Wcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking labels\n",
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "JTjxLvvm1h3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(x)[:, 0]"
      ],
      "metadata": {
        "id": "W9wPVob51mEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "tahxFGOA1szr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.scatterplot(x=np.array(x)[:, 0], y=np.array(x)[:, 2], hue=kmeans.labels_)\n",
        "#sns.scatterplot(np.array(x)[:, 0], np.array(x)[:, 2], c = kmeans.labels_)"
      ],
      "metadata": {
        "id": "q6Q3d8qC1v1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Find Optimum Numbers of Clusters\n",
        "\n",
        "\n",
        "*   Elbow Method\n",
        "*   silhouette Score"
      ],
      "metadata": {
        "id": "KJovWliV13Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Determine the optimal number of clusters"
      ],
      "metadata": {
        "id": "HXb1nQtx2rvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the sum of squared errors for each K value\n",
        "Sum_of_Squared_Errors = []\n",
        "\n",
        "# Iterate over range of K values and compute SSE for each value\n",
        "for k in range(1, 10):\n",
        "    # Initialize the k-means model with the current value of K\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    # Fit the model to the data\n",
        "    kmeans.fit(x)\n",
        "    # Compute the sum of squared errors for the model\n",
        "    Sum_of_Squared_Errors.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the SSE values against the range of K values\n",
        "plt.plot(range(1, 10), Sum_of_Squared_Errors)\n",
        "plt.title('Elbow Method for K-Means Clustering')\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Sum of Squared Errors (SSE)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m03rt4f52vOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have narrowed down the range of possible number of clusters to be between 4 to 7, as the slope of the elbow plot is steep at this range. To determine the optimal number of clusters, I will check the silhouette scores for each value in this range and choose the one with the highest score."
      ],
      "metadata": {
        "id": "QB_rBYsh2-I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def silhouette_score_analysis(n):\n",
        "  silhouette_avg = []\n",
        "  for k in range(2, n):\n",
        "    # Initialize the k-means model with the current value of k\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    # Fit the model to the data\n",
        "    kmeans.fit(x)\n",
        "    # Predict the cluster labels for each point in the data\n",
        "    labels = kmeans.labels_\n",
        "    preds = kmeans.fit_predict(x)\n",
        "    # Compute the silhouette score for the model\n",
        "    score = silhouette_score(x, labels)\n",
        "    silhouette_avg.append(score)\n",
        "\n",
        "    score = silhouette_score(x, preds, metric='euclidean')\n",
        "    print (\"For n_clusters = {}, silhouette score is {}\".format(k, score))\n",
        "\n",
        "    visualizer = SilhouetteVisualizer(kmeans)\n",
        "\n",
        "    visualizer.fit(x) # Fit the training data to the visualizer\n",
        "    visualizer.poof() # Draw/show/poof the data\n",
        ""
      ],
      "metadata": {
        "id": "JIo1g5HO3KTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_score_analysis(10)"
      ],
      "metadata": {
        "id": "lvdkDpZr3WXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above silhouette score visualization, we can observe that all values range between 0 and 1, indicating that the clusters formed are well-defined and separated, and hence are considered good."
      ],
      "metadata": {
        "id": "nnIv7OPD7N-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score method to find the optimal value of k**"
      ],
      "metadata": {
        "id": "v_IQhOOx7Rgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette score method to find the optimal value of k\n",
        "# Initialize a list to store the silhouette score for each value of k\n",
        "silhouette_avg = []\n",
        "\n",
        "# Define a list of possible number of clusters\n",
        "range_n_clusters = [2, 3, 4, 5, 6, 7]\n",
        "\n",
        "# Loop through each value of k\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Initialize the k-means model with the current value of k\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
        "    # Fit the model to the data\n",
        "    kmeans.fit(x)\n",
        "    # Predict the cluster labels for each point in the data\n",
        "    labels = kmeans.labels_\n",
        "    # Compute the silhouette score for the model\n",
        "    score = silhouette_score(x, labels)\n",
        "    # Append the silhouette score to the list of scores\n",
        "    silhouette_avg.append(score)\n",
        "    # Print the silhouette score for the current value of k\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
        "\n",
        "# Plot the Silhouette analysis\n",
        "plt.plot(range_n_clusters, silhouette_avg)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette analysis For Optimal k - KMeans clustering')\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "u3fmX-gK7fZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the 7 has the highest silhouette score. So we will take number of clusters as \"7\""
      ],
      "metadata": {
        "id": "Oojm7WU68ieN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Initialize the KMeans model with the chosen number of clusters\n",
        "kmeans_model = KMeans(n_clusters=7, random_state=42)\n",
        "kmeans_model.fit(x)               # Fit the Algorithm\n",
        "y_kmeans = kmeans_model.predict(x)# Predict on the model\n",
        "labels = kmeans_model.labels_     # Get the cluster labels for each point in the data\n",
        "unique_labels = np.unique(labels) # Get the unique cluster labels\n",
        ""
      ],
      "metadata": {
        "id": "jdKFcDWD7zuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a k-means cluster number attribute\n",
        "df['kmeans_cluster'] = labels\n",
        ""
      ],
      "metadata": {
        "id": "wz98EhNv729x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "2s4-6XLG8AsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dict_kmeans = evaluate_clustering_model(kmeans_model, x, y_kmeans)"
      ],
      "metadata": {
        "id": "6mPGtRin8EEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_clustering_scores(scores_dict_kmeans)"
      ],
      "metadata": {
        "id": "7g-OomkH8Gwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of the data colored by cluster label\n",
        "plt.figure(figsize=(8, 6), dpi=120)\n",
        "for i in unique_labels:\n",
        "    plt.scatter(x[labels == i, 0], x[labels == i, 1], s=20, label='Cluster {}'.format(i))\n",
        "plt.scatter(kmeans_model.cluster_centers_[:, 0], kmeans_model.cluster_centers_[:, 1], s=100, marker='x', c='black', label='Cluster centers')\n",
        "plt.title('KMeans clustering with {} clusters'.format(len(unique_labels)))\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jgzN504C8On-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans_wordcloud(cluster_number, column_name):\n",
        "    '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "    # Filter the data by the specified cluster number and column name\n",
        "    df_wordcloud = df[['kmeans_cluster', column_name]].dropna()\n",
        "    df_wordcloud = df_wordcloud[df_wordcloud['kmeans_cluster'] == cluster_number]\n",
        "\n",
        "    # Combine all text documents into a single string\n",
        "    text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "    # Create the word cloud\n",
        "    wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"white\").generate(text)\n",
        "\n",
        "    # Convert the wordcloud to a numpy array\n",
        "    image_array = wordcloud.to_array()\n",
        "\n",
        "    # Return the numpy array\n",
        "    return image_array"
      ],
      "metadata": {
        "id": "oouUc5Mh8UOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=6, ncols=7, figsize=(20, 15))\n",
        "\n",
        "for i in range(7):\n",
        "    for j, col in enumerate(['description', 'cast', 'director', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(kmeans_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g3P71__n8ZWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2) HIERARCHICAL CLUSTERING"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "dend = shc.dendrogram(shc.linkage(x, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Netflix Shows')\n",
        "plt.ylabel('Distance')\n",
        "plt.axhline(y= 5, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eB5srNe3819P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "dend = shc.dendrogram(shc.linkage(x, method='ward'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Netflix Shows')\n",
        "plt.ylabel('Distance')\n",
        "plt.axhline(y= 5, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R-TY1zIj85tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Horizontal line cutting 5 branches. So I have choose number of clusters as 5."
      ],
      "metadata": {
        "id": "No_5NX2S883D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AgglomerativeClustering**"
      ],
      "metadata": {
        "id": "WxVMe72-9BNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2  Implementation\n",
        "# Initialize the hierarchical model with the chosen number of clusters\n",
        "hierarchical_model = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
        "y_hierarchical = hierarchical_model.fit_predict(x)# Fit and predict on the model\n",
        "hierarchical_labels = hierarchical_model.labels_  # Get the cluster labels for each point in the data\n",
        "unique_labels_h = np.unique(hierarchical_labels)  # Get the unique cluster labels\n",
        "silhouette_avg = silhouette_score(x, hierarchical_labels)   # Calculate the silhouette score\n",
        "print(\"The average silhouette_score is :\", silhouette_avg)\n",
        ""
      ],
      "metadata": {
        "id": "lrC6Yml19OnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['hierarchical_cluster'] = hierarchical_labels\n",
        ""
      ],
      "metadata": {
        "id": "DntaMQhj9Q5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dict_hierarchical = evaluate_clustering_model(hierarchical_model, x, y_hierarchical)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_clustering_scores(scores_dict_hierarchical)"
      ],
      "metadata": {
        "id": "UDLyy_YU9eC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of the data colored by cluster label\n",
        "plt.figure(figsize=(8, 6), dpi=120)\n",
        "for i in unique_labels_h:\n",
        "    plt.scatter(x[hierarchical_labels == i, 0], x[hierarchical_labels == i, 1], s=20, label='Cluster {}'.format(i))\n",
        "#plt.scatter(hierarchical_model.cluster_centers_[:, 0], hierarchical_model.cluster_centers_[:, 1], s=100, marker='x', c='black')\n",
        "plt.title('Hierarchical clustering with {} clusters'.format(len(unique_labels_h)))\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kWuBKUA29md8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hierarchical_wordcloud(cluster_number, column_name):\n",
        "\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df[['hierarchical_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['hierarchical_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"white\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud\n",
        ""
      ],
      "metadata": {
        "id": "kg3T65H99pZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=6, ncols=5, figsize=(15, 15))\n",
        "\n",
        "for i in range(5):\n",
        "    for j, col in enumerate(['description', 'cast', 'director', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(hierarchical_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3HcjuBTK9r4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3) RECOMMENDATION SYSTEM"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Create a TF-IDF vectorizer object and transform the text data\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(new_df['normalized'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "def generate_recommendations(title, cosine_sim=cosine_sim, data=new_df):\n",
        "    # Get the index of the input title in the programme_list\n",
        "    programme_list = data['title'].to_list()\n",
        "    index = programme_list.index(title)\n",
        "\n",
        "    # Create a list of tuples containing the similarity score and index\n",
        "    # between the input title and all other programmes in the dataset\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort the list of tuples by similarity score in descending order\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
        "\n",
        "    # Get the recommended movie titles and their similarity scores\n",
        "    recommend_index = [i[0] for i in sim_scores]\n",
        "    rec_movie = data['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1], 4) for i in sim_scores]\n",
        "\n",
        "    # Create a pandas DataFrame to display the recommendations\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie, rec_score)),\n",
        "                             columns=['Recommended movie', 'Similarity score (0-1)'])\n",
        "\n",
        "    return rec_table"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_recommendations('Stranger Things')"
      ],
      "metadata": {
        "id": "5eoXAPuD-PB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_recommendations('Phir Hera Pheri')"
      ],
      "metadata": {
        "id": "kWv8dFxX-Re1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_recommendations('Black Panther')"
      ],
      "metadata": {
        "id": "lOi4qBt3-VWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ML Model - 4) DBSCAN Clustering**"
      ],
      "metadata": {
        "id": "Fh4ol0X9-pV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "# Create an instance of DBSCAN with specified hyperparameters\n",
        "dbscan_model = DBSCAN(eps=0.7, min_samples=3)\n",
        "dbscan_model.fit(x) # Fit the model to the input data\n",
        "y_dbscan = dbscan_model.labels_ # Get the predicted cluster labels for the input data\n",
        "dbscan_labels = dbscan_model.labels_\n",
        "unique_labels_dbscan = np.unique(dbscan_labels)\n",
        "print(y_dbscan)   # Print the predicted labels\n",
        ""
      ],
      "metadata": {
        "id": "UZ3zv7zC-1TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['dbscan_cluster'] = dbscan_labels\n",
        ""
      ],
      "metadata": {
        "id": "yGbSocdO-567"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "YhdlnjFQ-9na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores_dict_dbscan = evaluate_clustering_model(dbscan_model, x, y_dbscan)"
      ],
      "metadata": {
        "id": "VvDOCubW_E1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_clustering_scores(scores_dict_dbscan)"
      ],
      "metadata": {
        "id": "lbAY-VZi_LLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of the data colored by cluster label\n",
        "plt.figure(figsize=(8, 6), dpi=120)\n",
        "for i in unique_labels_dbscan:\n",
        "    plt.scatter(x[dbscan_labels == i, 0], x[dbscan_labels == i, 1], s=20, label='Cluster {}'.format(i))\n",
        "plt.title('DBSCAN clustering with {} clusters'.format(len(unique_labels_dbscan)))\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "a0fmTsjz_OVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dbscan_wordcloud(cluster_number, column_name):\n",
        "\n",
        "  '''function for Building a wordcloud for the movie/shows'''\n",
        "\n",
        "  # Filter the data by the specified cluster number and column name\n",
        "  df_wordcloud = df[['dbscan_cluster', column_name]].dropna()\n",
        "  df_wordcloud = df_wordcloud[df_wordcloud['dbscan_cluster'] == cluster_number]\n",
        "\n",
        "  # Combine all text documents into a single string\n",
        "  text = \" \".join(word for word in df_wordcloud[column_name])\n",
        "\n",
        "  # Create the word cloud\n",
        "  wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color=\"white\").generate(text)\n",
        "\n",
        "  # Return the word cloud object\n",
        "  return wordcloud"
      ],
      "metadata": {
        "id": "pPKBfGu1_RwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=6, ncols=5, figsize=(15, 15))\n",
        "\n",
        "for i in range(5):\n",
        "    for j, col in enumerate(['description', 'cast', 'director', 'listed_in', 'country', 'title']):\n",
        "        axs[j][i].imshow(dbscan_wordcloud(i, col))\n",
        "        axs[j][i].axis('off')\n",
        "        axs[j][i].set_title(f'Cluster {i}, {col}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "_c3TcAVv_U07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which Evaluation metrics did you consider for a positive business impact and why?\n",
        "\n",
        "1) Silhouette score\n",
        "\n",
        "Silhouette score is a popular evaluation metric for clustering algorithms. It measures how well each data point fits into its assigned cluster compared to other clusters. The score ranges from -1 to 1, with a higher score indicating better-defined clusters.\n",
        "\n",
        "Silhouette score is a useful metric for a positive business impact because it can help identify the optimal number of clusters for a dataset. This, in turn, can help companies make data-driven decisions and allocate resources more efficiently based on the distinct patterns and characteristics of each cluster.\n",
        "\n",
        "2) Calinski-Harabasz score\n",
        "\n",
        "The Calinski-Harabasz score, also known as the variance ratio criterion, is a measure of the ratio between the within-cluster dispersion and the between-cluster dispersion. It is calculated by taking the ratio of the sum of squares between groups to the sum of squares within groups, multiplied by the ratio of the number of observations to the number of clusters minus one.\n",
        "\n",
        "In other words, the Calinski-Harabasz score measures how well separated the clusters are in the data and how compact the clusters are internally. A higher score indicates that the clusters are well separated and compact, while a lower score indicates that the clusters are not well separated or are not compact.\n",
        "\n",
        "3) Davies-Bouldin score\n",
        "\n",
        "The Davies-Bouldin score is a measure of the average similarity between each cluster and its most similar cluster, compared to the average dissimilarity between each cluster and its least similar cluster. It is calculated by taking the sum of the ratios of the within-cluster scatter and the between-cluster distances, divided by the number of clusters.\n",
        "\n",
        "In other words, the Davies-Bouldin score measures how well separated the clusters are in the data and how distinct they are from each other. A lower score indicates that the clusters are well separated and distinct, while a higher score indicates that the clusters are not well separated or are not distinct from each other."
      ],
      "metadata": {
        "id": "U5aJsHdL_cm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Storing metrics in order to make dataframe of metrics\n",
        "Model          = ['K-Means Clustering', 'Hierarchical Clustering', 'DBSCAN Clustering']\n",
        "S_score  = [0.0051, 0.0005, -0.0148]\n",
        "CH_score = [22.0021, 18.1425, 2.8595]\n",
        "DB_score = [10.7600, 12.1666, 1.4252]\n",
        "No_of_cluster = [7, 5, 17]\n",
        "# Create dataframe from the lists\n",
        "data = {'Model' : Model,\n",
        "        'Number of clusters': No_of_cluster,\n",
        "        'silhouette_score'  : S_score,\n",
        "        'calinski_harabasz_score': CH_score,\n",
        "        'davies_bouldin_score': DB_score}\n",
        "Metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "Metric_df"
      ],
      "metadata": {
        "id": "klGtu7Ab_vmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating several machine learning models, including K-Means Clustering, Hierarchical Clustering - Agglomerative, DBSCAN Clustering, and Recommender System, we concluded that K-Means Clustering was the most suitable prediction model for our project.\n",
        "\n",
        "We selected K-Means Clustering as our final model because it demonstrated high accuracy and computational efficiency on our evaluation dataset. It was able to cluster similar movies and TV shows based on their shared attributes, which enabled us to provide better recommendations to our users. Additionally, K-Means Clustering was relatively simple to implement and maintain, making it a practical choice for our project.\n",
        "\n",
        "Although Hierarchical Clustering - Agglomerative and DBSCAN Clustering showed promise, they were computationally intensive and required more processing power and time to execute. The Recommender System, on the other hand, had limitations in its ability to cluster movies and TV shows based on shared attributes and relied heavily on user behavior data to make recommendations.\n",
        "\n",
        "We selected K-Means Clustering as our final prediction model because it had the best calinski_harabasz_score and Davies-Bouldin score, which were higher and lower, respectively, than the scores obtained by other models. Additionally, the model produced a good silhouette score.\n",
        "\n",
        "In summary, we chose K-Means Clustering as our final prediction model for its accuracy, efficiency, and practicality in providing recommendations to our users."
      ],
      "metadata": {
        "id": "tsXqMVKl_2Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "import pickle\n",
        "filename='NETFLIX MOVIES AND TV SHOWS CLUSTERING.pkl'\n",
        "\n",
        "# serialize process (wb=write byte)\n",
        "pickle.dump(kmeans_model,open(filename,'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# unserialize process (rb=read byte)\n",
        "kmeans_model= pickle.load(open(filename,'rb'))\n",
        "\n",
        "# Predicting the unseen data\n",
        "kmeans_model.predict(x)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if we are getting the same predicted values\n",
        "y_kmeans\n",
        ""
      ],
      "metadata": {
        "id": "V3GzyZRvASDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION FROM EDA:**\n",
        "\n",
        "1.On the Netflix platform, there are more movies available than TV shows. Netflix primarily features content for mature audiences, with a majority of it having a TV-MA rating.\n",
        "\n",
        "2.The countries with the highest number of productions available on Netflix are the United States, India, and the United Kingdom.\n",
        "\n",
        "3.Since its establishment in 2008, Netflix's content library has experienced a consistent increase.\n",
        "\n",
        "4.The most common genres of content on Netflix are Dramas, Comedies, and Documentaries.\n",
        "\n",
        "5.According to the Wordcloud visualization of movie descriptions, some of the most frequently used words in Netflix movie descriptions are love, family, young, life, and world.\n",
        "\n",
        "6.The correlation heatmap indicates a moderate positive correlation between a movie's release year and its duration.\n",
        "\n",
        "7.The pairplot reveals several interesting patterns between variables, such as a strong positive correlation between the number of reviews and the year of release, and a negative correlation between a movie's rating and its duration.\n",
        "\n",
        "**CONCLUSION FROM MODEL IMPLEMENTATION:**\n",
        "\n",
        "1.Based on the attributes of director, cast, country, genre, rating, and description, the data was clustered.\n",
        "\n",
        "2.To tokenize, preprocess, and vectorize the values in the attributes, TFIDF vectorizer was used, resulting in a total of 10,000 attributes.\n",
        "\n",
        "3.In order to capture more than 95% of the variance, Principal Component Analysis (PCA) was utilized to reduce the dimensionality of the data.\n",
        "\n",
        "4.Using the elbow method and Silhouette score analysis, the optimal number of clusters for the K-Means Clustering algorithm was determined to be 7.\n",
        "\n",
        "5.The optimal number of clusters for the Agglomerative clustering algorithm was determined to be 5 based on the dendrogram visualization.\n",
        "\n",
        "6.Utilizing cosine similarity, a content-based recommender system was constructed and will provide 10 recommendations to the user based on the type of show they previously watched.\n",
        "\n",
        "7.DBSCAN clustering was utilized, and it identified 17 optimal clusters with a low metric score."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}